%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Cleese Assignment (For Students)
% LaTeX Template
% Version 2.0 (27/5/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Author:
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{article}

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

% Required
\newcommand{\assignmentQuestionName}{Question} % The word to be used as a prefix to question numbers; example alternatives: Problem, Exercise
\newcommand{\assignmentClass}{Introduction to Machine Learning (CSC2515)} % Course/class
\newcommand{\assignmentTitle}{QnA\ \#1} % Assignment title or name
\newcommand{\assignmentAuthorName}{Sumant Bagri (1005644313)} % Student name

% Optional (comment lines to remove)
%\newcommand{\assignmentClassInstructor}{Jones 10:30am} % Intructor name/time/description
\newcommand{\assignmentDueDate}{Monday,\ Octorber\ 03,\ 2022} % Due date

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\maketitle % Print the title page

\thispagestyle{empty} % Suppress headers and footers on the title page

\newpage

%----------------------------------------------------------------------------------------
%	QUESTION 1
%----------------------------------------------------------------------------------------

\begin{question}

\questiontext{What are the effects of having different weights for the data points in the training set of a k-NN model and how do we determine these weights?}

\answer{Results generated using the basic k-NN approach assumes that all datapoints neighbouring the test data have equal contribution in determining the output label (in case of classification) of the test input. This implies that the model will run through all the sample datapoints to find the k-nearest-neighbours and then assign the majority class to the test data. Therefore, the computational complexity of k-NN increases monotonically with the size of the training dataset. ($\mathcal{O}(n \cdot d) + \mathcal{O}(n\log{n})$: n=training data size; d=input dimension)\\

However, if we assign weights to each sample of the training data, we can reduce the number of points k-NN has to consider while establishing nearest-neighbours. This improves the computational complexity significantly and will also perform better with high dimensional datasets. The procedure to determine the weights is as follows:
\begin{itemize}
	\item Cluster the training data (k-means) to generate representative points (which are cluster-centers) for each category. This reduces the size of the training data significantly
	\item Assign weights to the cluster centers equal to the fraction of points of a particular category within a cluster to the total number of points in that category  
\end{itemize}
}

\end{question}

%----------------------------------------------------------------------------------------
%	QUESTION 2
%----------------------------------------------------------------------------------------

\begin{question}

\questiontext{What is the time complexity of growing a decision tree using Information Gain as the splitting criteria?}


\answer{
Let $X$ be the list of attributes for the dataset $D$. Information Gain for splitting dataset $D$ on a particular attribute $A \in X$ can be defined as follows:

\begin{equation}\label{eq:IGa}
	IG(D;A) = H(D) - \sum_{x \in A} \frac{\lvert D_x \rvert}{\lvert D \rvert}H(D|A = x))
\end{equation}

where $H(D)$ calculates the entropy in dataset $D$ $\forall$ output categories $c_i \in C$ as follows:

\begin{equation}\label{eq:ent}
	H(D) = -\sum_{i=1}^{\lvert C \rvert}P_D(c_i)\log{P_D(c_i)}
\end{equation}

If there are $n$ attributes (dimensionality of the data is $n$) the algorithm has to go through all the datapoints in $D$ for each of the $n$ attributes to calculate individual information gains. It then, greedily, selects the attribute with the best information gain and splits on it. This results in a time complexity of $\mathcal{O}(\lvert D \rvert \cdot n)$  at each level. Assuming that we split on an attribute only once, the maximum number of levels in the tree will correspond to the number of attributes $n$. Hence, the total time complexity of building the tree is $\mathcal{O}(m \cdot n^2)$
}

%--------------------------------------------

\end{question}


\end{document}
